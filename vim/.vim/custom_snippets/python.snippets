###############################################################################
#                                   Logging                                   #
###############################################################################

snippet p "Print"
print("$0")
endsnippet

snippet pf "Print Formatted String"
print(f"${1}")
endsnippet

snippet log "Log Basic Setup"
logging.basicConfig(level=logging.${1:DEBUG})
$0
endsnippet

snippet l "Log"
logging.info("${1}")
endsnippet

###############################################################################
#                                   General                                   #
###############################################################################

snippet write_csv "Write to CSV"
import csv

with open('output.csv', 'w') as f:
    w = csv.writer(f)
    w.writerow([$0])
endsnippet

###############################################################################
#                                   Pandas                                    #
###############################################################################

snippet pd "Import Pandas as `pd`"
import pandas as pd
endsnippet

snippet csv "Pandas Read CSV"
pd.read_csv("${1:filename}", header=None)
$0
endsnippet

snippet tsv "Pandas Read TSV"
pd.read_csv("${1:filename}", sep="\t", header=None, names=["word", "score"])
$0
endsnippet

snippet json "Pandas Newline Delimited JSON"
df = pd.read_json("${1:filename}", lines=True)
$0
endsnippet

snippet dfglob "Load Multiple Files to Dataframe"
from glob import glob
import pandas as pd

dfs = []
for f in glob("$0"):
    df = pd.read_json(f, lines=True)
df = pd.concat(dfs)
endsnippet

snippet pcat "Concatenate dataframes"
pd.concat([$0])
endsnippet

snippet plot "Generate matplotlib to file"
axes = df.plot(subplots=True)[0]
axes.set_title("${1:Plot Title}")
fig = axes.get_figure()
fig.savefig("${0:output.png}")
endsnippet

###############################################################################
#                                    Spark                                    #
###############################################################################

snippet ss "Spark Session"
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("${0:Spark Session}").getOrCreate()
sc = spark.sparkContext
endsnippet

snippet sdf "Manually Instantiate Spark DataFrame"
df = spark.createDataFrame(
    [
        (1, 'foo'),
        (2, 'bar'),
        (3, 'baz'),
    ],
    ['id', 'msg']
)
endsnippet

snippet window "Spark Window"
w = Window.partitionBy("${1:column}")
df = df.withColumn("${2:column_aggregate}", F.max("${1:column}").over(w))
endsnippet
